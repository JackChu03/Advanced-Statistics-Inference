---
title: "Homework 4: Binary Classification"
author: "Jack Chu"
output: html_document
editor_options: 
  chunk_output_type: console
---

There are four questions (30 total points) in this assignment. The minimum increment is 1 point. Please type in your answers directly in the R Markdown file. After completion, **successfully** knitr it as an html file. Submit <span style="color:red">**both**</span> the html file and the R Markdown file via Canvas. Please name the R Markdown file in the following format: LastName_FirstName_HW4.Rmd, e.g. Zhao_Zifeng_HW4.Rmd.


## Adult Income Dataset [30 points]
The adult income dataset contains information about 9755 adults from the 2010 U.S. Census database. The data is stored in `Adult_Income.csv`. It contains 8 variables, `age`, `workclass`, `education_num`, `marital_status`, `capital_gain`, `capital_loss`, `hours_per_week`, and `income_50k`. We would like to build several statistical models to predict `income_50k` (i.e. whether income is higher than 50k or not) of a person with given personal information. The data description is as follows.

+ `age`: Age in years
+ `workclass`: Type of employment the person has
+ `education_num`: Education in years
+ `marital_status`: Marital status
+ `capital_gain`: Capital gain in the past year
+ `capital_loss`: Capital loss in the past year
+ `hours_per_week`: Average working hours per week
+ `income_50k`: A factor with levels Yes and No indicating whether the income higher than 50k or not


###  **Q1 [3 points]** Data Partition
**Q1(a) [1 points]**
Let's correctly read in the data in `Adult_Income.csv` and name it as `total_data`. 
```{r Q1(a)}
## Write code solution to Q1(a) here
rm(list=ls())
total_data <- read.csv("~/Downloads/Adult_Income.csv", header=T, stringsAsFactors=T)
```


**Q1(b) [2 points]**
Let's partition the data in `total_data` into training **(60%)** and test data **(40%)** and store them as `R` objects `train_data` and `test_data` respectively. Use random seed **`set.seed(7)`**!
```{r Q1(b)}
## Write code solution to Q1(b) here
set.seed(7)
total_obs <- dim(total_data)[1]
## Data Partition: Training v.s. Test split
train_data_indices <- sample(1:total_obs, 0.6*total_obs)
train_data <- total_data[train_data_indices,]
test_data <- total_data[-train_data_indices,]
```


### **Q2 [8 points]** Logistic Regression and GAM
**Q2(a) [3 points]**
Fit a logistic regression model of `income_50k` w.r.t. all 7 predictors using the **training data**, name it `lm1`.
```{r Q2(a)}
## Write code solution to Q2(a) here
lm1 <- glm(income_50k~., family='binomial', data=train_data)
summary(lm1)
```


**Q2(b) [5 points]**
Fit a GAM of `income_50k` w.r.t. all 7 predictors using the **training data**, name it `gam1`. Let's use splines with **df=4** for all 5 numerical predictors, which include `age`, `education_num`, `capital_gain`, `capital_loss` and `hours_per_week`.
```{r Q2(b)}
library(gam)
## Write code solution to Q2(b) here
gam1 <- gam(income_50k~ s(age) + s(education_num) + s(capital_gain) + s(capital_loss) + s(hours_per_week)+workclass+marital_status, family='binomial', data=train_data)
```


### **Q3 [9 points]** Neural Networks
Fit an NN of **standardized** `income_50k` w.r.t. all 7 predictors using the **training data**, name it `nn1`. For the architecture of NN, let's use one hidden layer with 6 hidden units.

**Q3(a) [2 points]**
Let's generate the **training dataset** that are needed for the estimation of NN using the function `model.matrix()` and store it in `x_train_nn`. In addition, use the `scale()` function to standardize the predictors by centering with mean and scaling with sd.
```{r Q3(a)}
## Write code solution to Q3(a) here
x_train_nn <- model.matrix(~age + education_num + capital_gain + capital_loss + hours_per_week + workclass+ marital_status, data=train_data)[,-1]
# standardization
x_mean <- apply(x_train_nn, 2, mean)
x_sd <- apply(x_train_nn, 2, sd)
x_train_nn <- scale(x_train_nn, center=x_mean, scale=x_sd)
```


**Q3(b) [1 points]**
Let's further combine the dependent variable `income_50k` with the standardized predictors `x_train_nn` generated in Q3(a).
```{r Q3(b)}
## Write code solution to Q3(b) here
x_train_nn <- cbind.data.frame(train_data$income_50k, x_train_nn)
colnames(x_train_nn)[1] <- 'income_50k'
```


**Q3(c) [2 points]**
Let's generate the **test dataset** that are needed for the out-of-sample prediction evaluation of NN using the function `model.matrix` and store it in `x_test_nn`. Use the `scale()` function to standardize the predictors by centering with mean and scaling with sd as in Q3(a).
```{r Q3(c)}
## Write code solution to Q3(c) here
x_test_nn <- model.matrix(~age + education_num + capital_gain + capital_loss + hours_per_week + workclass+ marital_status, data=test_data)[,-1]
# standardization
x_test_nn <- scale(x_test_nn, center=x_mean, scale=x_sd)
```


**Q3(d) [4 points]**
Let's fit an NN that has one hidden layer with 6 hidden units. Make sure to use random seed **`set.seed(7)`**! Note that since some categorical variables have a number of different levels, for convenience, let's use the shortcut formula `Y~.` in the function `neuralnet()`. (You can also use the `Y~X1+X2...+Xp` formula if you want.)
```{r Q3(d)}
## Write code solution to Q3(d) here
library(neuralnet)
nn1 <- neuralnet(income_50k=='Yes'~., data=x_train_nn, hidden=c(6), linear.output=F)
```


### **Q4 [10 points]** Model Evaluation (Prediction)
**Q4(a) [2 points]**
Use `lm1`, `gam1` and `nn1` to generate probability predictions for `income_50k` on the test data and store the predicted probability in `lm1_pred`, `gam1_pred` and `nn1_pred` respectively.
```{r Q4(a)}
## Write code solution to Q4(a) here
lm1_pred <- predict(lm1, newdata = test_data, type='response')
gam1_pred <- predict(gam1, newdata = test_data, type='response')
nn1_pred <- predict(nn1, newdata=x_test_nn)[,1]
```


**Q4(b) [3 points]**
Use the `R` package `caret` to evaluate the prediction performance of `lm1`, `gam1` and `nn1`. What are the TP for `lm1`, `gam1` and `nn1`?
```{r Q4(b)}
library(caret)
## Write code solution to Q4(b) here
lm1_acc <- confusionMatrix(factor(ifelse(lm1_pred>0.5, 'Yes', 'No')), test_data$income_50k, positive='Yes')
gam1_acc <- confusionMatrix(factor(ifelse(gam1_pred>0.5, 'Yes', 'No')), test_data$income_50k, positive='Yes')
nn1_acc <- confusionMatrix(factor(ifelse(nn1_pred>0.5, 'Yes', 'No')), test_data$income_50k, positive='Yes')

print(lm1_acc)
print(gam1_acc)
print(nn1_acc)
```

Answer: TP for lm1 is 536. TP for gam1 is 532. TP for nn1 is 540.  


**Q4(c) [1 points]**
Which statistical model has the best sensitivity, `lm1` or `gam1` or `nn1`? Give the model and the corresponding sensitivity value.

Answer: 
nn1 has the best sensitivity and it has sensitivity value 0.5850.

**Q4(d) [2 points]**
Use the `R` package `caret` to generate the lift charts of `lm1`, `gam1` and `nn1`. Make sure to set the `cuts` argument in the `lift()` function as **`cuts=100`** to save computational time.
```{r Q4(d)}
## Write code solution to Q4(d) here
lift_chart <- lift(test_data$income_50k~lm1_pred+gam1_pred+nn1_pred, class='Yes')
xyplot(lift_chart, auto.key=list(columns=4), main='Lift Chart')
```


**Q4(e) [2 points]**
Based on the lift chart, which statistical model performs the best in terms of identifying people with income > 50k?

Answer:
gam1
